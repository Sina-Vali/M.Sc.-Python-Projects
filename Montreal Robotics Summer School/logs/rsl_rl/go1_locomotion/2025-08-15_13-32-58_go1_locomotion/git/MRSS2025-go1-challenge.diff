--- git status ---
On branch master
Your branch is up to date with 'origin/master'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   config/training_params.yaml
	modified:   go1_challenge/isaaclab_tasks/go1_locomotion/go1_locomotion_env_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/config/training_params.yaml b/config/training_params.yaml
index 4a5ebd9..251fa88 100644
--- a/config/training_params.yaml
+++ b/config/training_params.yaml
@@ -1,7 +1,7 @@
 task: MRSS-Velocity-Go1-v0  # Name of the task.
 
 headless: true  # Run simulation without rendering a GUI (faster, better for training on servers)
-max_iterations: 500  # Adjust this to train for longer: 1.5k should give you a working policy for rough terrain
+max_iterations: 700  # Adjust this to train for longer: 1.5k should give you a working policy for rough terrain
 
 seed: 1 # Performance should be somewhat consistent across seeds
 
@@ -11,14 +11,14 @@ video_interval: 10  # Interval between video recordings (in steps).
 
 run_name: go1_locomotion  # Run name suffix to the log directory.
 resume: false  # Whether to resume from a checkpoint.
-#load_run: LOAD_RUN   # Name of the run folder to resume from.
-#checkpoint: CHECKPOINT  # Checkpoint file to resume from.
+# load_run: 2025-08-14_12-45-03_go1_locomotion_first_try   # Name of the run folder to resume from.
+# checkpoint: model_1499.pt  # Checkpoint file to resume from.
 logger: wandb  # Logger module to use. {neptune, wandb, tensorboard}
 log_project_name: MRSS25  # Name of the logging project when using wandb or neptune.
 
 env:
   scene:
-    num_envs: 1  # Number of environments to generate terrain for and run in parallel.
+    num_envs: 2024  # Number of environments to generate terrain for and run in parallel.
 
     terrain:
       max_init_terrain_level: 5  # Maximum initial difficulty level of terrains for curriculum learning
@@ -35,25 +35,25 @@ env:
         # See the different possible terrains here https://isaac-sim.github.io/IsaacLab/main/source/api/lab/isaaclab.terrains.html#terrain-generator
         sub_terrains:
           flat: 
-            proportion: 1.0
+            proportion: 0.4
 
           pyramid_stairs:
-            proportion: 0.0
+            proportion: 0.1
 
           pyramid_stairs_inv:
-            proportion: 0.0
+            proportion: 0.1
 
           boxes:
-            proportion: 0.0
+            proportion: 0.1
 
           random_rough: 
-            proportion: 0.0
+            proportion: 0.1
 
           pyramid_slope: 
-            proportion: 0.0
+            proportion: 0.1
 
           pyramid_slope_inv: 
-            proportion: 0.0
+            proportion: 0.1
         
       # terrain_generator: 
       #   sub_terrains: 'null'
@@ -68,25 +68,31 @@ env:
     
   rewards:
     action_rate_l2:
-      weight: -0.1  # Penalty for rapid changes in actions (encourages smoothness)
+      weight: -0.05  # Penalty for rapid changes in actions (encourages smoothness)
     ang_vel_xy_l2:
       weight: -0.05  # Penalty for angular velocity in x and y directions (helps stability)
     dof_acc_l2:
       weight: -2.5e-07  # Penalty for joint accelerations (encourages smoother motion)
     dof_pos_limits:
-      weight: 0.0  # Penalty for exceeding joint limits (disabled here)
+      weight: -1.0  # Penalty for exceeding joint limits (disabled here)
     dof_torques_l2:
-      weight: -0.0002  # Penalty for large joint torques (encourages energy efficiency)
+      weight: -0.00002  # Penalty for large joint torques (encourages energy efficiency)
     feet_air_time:
-      weight: 0.01  # Reward for time feet spend in the air (encourages stepping/gait)
+      weight: 0.001  # Reward for time feet spend in the air (encourages stepping/gait)
     flat_orientation_l2:
-      weight: 0.0  # Penalty for orientation deviation from flat (disabled here)
+      weight: -1.5  # Penalty for orientation deviation from flat (disabled here)
     lin_vel_z_l2:
-      weight: -2.0  # Penalty for vertical (z-axis) linear velocity (encourages flat motion)
+      weight: -1.5  # Penalty for vertical (z-axis) linear velocity (encourages flat motion)
     track_ang_vel_z_exp:
-      weight: 0.75  # Reward for matching desired angular velocity around z-axis (yaw control)
+      weight: 2.0  # Reward for matching desired angular velocity around z-axis (yaw control)
     track_lin_vel_xy_exp:
-      weight: 1.5  # Reward for matching desired linear velocity in x-y plane
+      weight: 2.5  # Reward for matching desired linear velocity in x-y plane
+    base_height_l2:
+      weight: -3.0 # Penalty for keeping the robot's body at a height of 0.32m
+    is_alive:
+      weight: 0.01 # Reward for staying alive (this should smooth learning)
+    is_terminated:
+      weight: -10.0 # Penalty for catastrophic failure where robot falls
 
   events:
     add_base_mass:
@@ -95,11 +101,11 @@ env:
 
     base_external_force_torque:
       params:
-        force_range: [ 0.0, 0.0 ]  # Range for randomizing the external force applied to the base
-        torque_range: [ 0.0, 0.0 ]  # Range for randomizing the external torque applied to the base
-
-    push_robot: 'null'  # Event to push the robot, set to None to disable
+        force_range: [ -5.0, 5.0 ]  # Range for randomizing the external force applied to the base
+        torque_range: [ -1.0, 1.0 ]  # Range for randomizing the external torque applied to the base
 
+    push_robot: # Event to push the robot, set to None to disable
+      interval_range_s: [20.0, 25.0]
 
 algorithm:
   class_name: 'PPO'  # Name of the RL algorithm to use (Proximal Policy Optimization)
@@ -117,4 +123,4 @@ algorithm:
   schedule: 'adaptive'  # Learning rate schedule type (adaptive vs fixed)
   symmetry_cfg: null  # Optional symmetry loss configuration (not used here)
   use_clipped_value_loss: True  # Use value function clipping in PPO
-  value_loss_coef: 1.0  # Coefficient for value function loss
+  value_loss_coef: 1.0  # Coefficient for value function loss
\ No newline at end of file
diff --git a/go1_challenge/isaaclab_tasks/go1_locomotion/go1_locomotion_env_cfg.py b/go1_challenge/isaaclab_tasks/go1_locomotion/go1_locomotion_env_cfg.py
index d17797b..209e87b 100644
--- a/go1_challenge/isaaclab_tasks/go1_locomotion/go1_locomotion_env_cfg.py
+++ b/go1_challenge/isaaclab_tasks/go1_locomotion/go1_locomotion_env_cfg.py
@@ -1,8 +1,3 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
 """
 Configuration for the locomotion velocity-tracking environment with Unitree Go1 robot. This environment is designed to
 for a Go1 robot to learn to walk and track velocity commands in a rough terrain environment.
@@ -227,7 +222,6 @@ class ObservationsCfg:
     policy: PolicyCfg = PolicyCfg()
     critic: CriticCfg = CriticCfg()
 
-
 @configclass
 class EventCfg:
     """Configuration for events."""
@@ -348,6 +342,10 @@ class RewardsCfg:
     flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=0.0)
     dof_pos_limits = RewTerm(func=mdp.joint_pos_limits, weight=0.0)
 
+    # Added rewards:
+    base_height_l2 = RewTerm(func=mdp.base_height_l2, weight=-10.0, params={"target_height": 0.60})
+    is_alive = RewTerm(func=mdp.is_alive, weight=0.1, params={})
+    is_terminated = RewTerm(func=mdp.is_terminated, weight=-2.0, params={})
 
 @configclass
 class TerminationsCfg: